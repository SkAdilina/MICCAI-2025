{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d051a6f-ca49-4ef9-95ec-766d6c825ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baabc7e1-2df9-477b-9b0f-ac9357719d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#print(torch.__version__)\n",
    "#print(torch.cuda.is_available())\n",
    "#print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab120db4-ca81-47c6-a580-4c235eee4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchio as tio\n",
    "from torchio import SubjectsDataset, SubjectsLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "import argparse\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28433a-e2d9-4932-9b75-c808419b41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using TorchIO images without a torchio.SubjectsLoader\")\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903cd4e-6608-44ae-903c-636ccb1fd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # Fixed for reproducibility\n",
    "print(\"Seed value:\", seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # For multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e07dc-8cfe-4a70-8d02-cf1110bf4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab3b11-9b61-4a59-b7ac-6ae4452bf748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c642bcf-e39b-433e-9cd1-4e64332e6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_used = \"/exp_OURS\"\n",
    "task_path = \"PATH_TO_TASK_DIR/Task_FreeSurfer_1-5T_longit\"\n",
    "task_name_exp = \"Longit-OURS\"\n",
    "print(\"-------\", task_name_exp, \"-------\")\n",
    "max_tp = 13\n",
    "\n",
    "if not os.path.exists(task_path + losses_used):\n",
    "    os.makedirs(task_path + losses_used)\n",
    "    os.makedirs(task_path + losses_used + '/models')\n",
    "    os.makedirs(task_path + losses_used + '/vol_loss_plots')\n",
    "    print(f\"Directory created: {task_path + losses_used}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {task_path + losses_used}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f88f55-662c-4007-aed8-e1c55b7b22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = task_path + losses_used + \"/fold_\" + str(global_fold) + \"_training_log.csv\"\n",
    "\n",
    "# Create CSV file with header at the beginning of the script\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Epoch\"])  # Writing header\n",
    "\n",
    "\n",
    "def log_epoch(epoch):\n",
    "    with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([epoch])  # Append epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a09ac-441e-4ebc-bcca-62be3e0ab0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "print(\"Started running the code at:\", now)\n",
    "print(\"############################################################################\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248adff-215d-499a-a22a-832abcc38ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import *\n",
    "from utils.model import *\n",
    "from utils.criterion import *\n",
    "from utils.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e2021-dce4-402b-a526-34bae20167fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=['Dice Loss', 'BCE Loss', 'hlfc Loss', 'loss1', 'Smoothness Loss OR loss2', 'Age Contraint Loss OR loss3', 'Mean Avg Volume loss', 'SDF loss', 'Total Loss', 'Epoch', 'Learning Rate']\n",
    "temp_fields=['Dice Loss', 'BCE Loss', 'hlfc Loss', 'loss1', 'Smoothness Loss OR loss2', 'Age Contraint Loss OR loss3', 'Mean Avg Volume loss', 'SDF loss', 'Total Loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32fd03-e3d7-441d-a51c-7723272f5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOSS USED --- SDF-VL-AC-SC on CROP-REG DS Experiment\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cdd2e9-be9e-4a79-a38b-01240f4daabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_losses(train_losses, val_losses, fold):\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "    \n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot (optional)\n",
    "    plt.savefig(task_path + losses_used +  '/train_val_losses_plot_' + str(fold) + '.png')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    plt.close()  # Close the plot to release memory\n",
    "\n",
    "def plot_all(loss_file_loc, fold):\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    train_all_losses = pd.read_csv(loss_file_loc)\n",
    "    val_all_losses = pd.read_csv(loss_file_loc.replace(\"all_losses_train_\", \"all_losses_val_\"))\n",
    "    \n",
    "    #print(\"train loss length\", len(train_all_losses))\n",
    "    #print(\"val loss length\", len(val_all_losses))\n",
    "    \n",
    "    epochs = range(1, len(train_all_losses) + 1)\n",
    "    #print(\"total epochs:\", epochs)\n",
    "\n",
    "    for cur_col in train_all_losses:\n",
    "\n",
    "        #print(cur_col)\n",
    "        \n",
    "        cur_train_loss = train_all_losses[cur_col]\n",
    "        cur_val_loss = val_all_losses[cur_col]\n",
    "    \n",
    "        plt.plot(epochs, cur_train_loss, 'b', label='Training Loss')\n",
    "        plt.plot(epochs, cur_val_loss, 'r', label='Validation Loss')\n",
    "        \n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(cur_col)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot (optional)\n",
    "        plt.savefig(task_path + losses_used + '/plot_losses_fold_' + str(fold) + '_' + cur_col +'.png')\n",
    "        \n",
    "        # Show plot\n",
    "        #plt.show()\n",
    "        plt.close()  # Close the plot to release memory\n",
    "\n",
    "def plot_all_iter(loss_file_loc, fold, type_loss):\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    train_all_losses = pd.read_csv(loss_file_loc)\n",
    "    #print(\"train loss length\", len(train_all_losses))\n",
    "    \n",
    "    epochs = range(1, len(train_all_losses) + 1)\n",
    "    #print(\"total epochs:\", epochs)\n",
    "\n",
    "    for cur_col in train_all_losses:\n",
    "\n",
    "        #print(cur_col)\n",
    "        \n",
    "        cur_train_loss = train_all_losses[cur_col]\n",
    "\n",
    "        if type_loss == \"train\":\n",
    "            plt.plot(epochs, cur_train_loss, 'b', label= 'Training Loss')\n",
    "        if type_loss == \"val\":\n",
    "            plt.plot(epochs, cur_train_loss, 'r', label= 'Validation Loss')\n",
    "        \n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel(cur_col)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot (optional)\n",
    "        plt.savefig(task_path + losses_used + '/plot_losses_iter_' + type_loss +'_fold_' + str(fold) + '_' + cur_col +'.png')\n",
    "        \n",
    "        # Show plot\n",
    "        #plt.show()\n",
    "        plt.close()  # Close the plot to release memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12676c-ad0c-4770-ac42-964d237aab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_from_csv(file_path):\n",
    "\n",
    "    with open(file_path, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = [row for row in reader]\n",
    "        data = np.array(data)\n",
    "\n",
    "    # Convert data to float (skipping the header)\n",
    "    data = data[1:].astype(float)\n",
    "    # Calculate column-wise mean\n",
    "    averages = np.mean(data, axis=0)\n",
    "    \n",
    "    return averages\n",
    "\n",
    "def get_average_from_csv_without_header(file_path):\n",
    "    \n",
    "    # Read the CSV file excluding the header\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    # Convert the DataFrame to a NumPy array\n",
    "    data = df.to_numpy(dtype=np.float64)\n",
    "    # Calculate column-wise mean\n",
    "    averages = np.mean(data, axis=0)\n",
    "    \n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf50120-fa27-4617-858c-8da7781c5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def train_batch(model, data, optimizer, criterion, loss_file_loc):\n",
    "\n",
    "    cur_fold = global_fold\n",
    "    print(\"################################## Train Loop: is fold\", cur_fold)\n",
    "\n",
    "    cur_pred = []\n",
    "    all_preds = []\n",
    "    ages = []\n",
    "    \n",
    "    model.train()\n",
    "    ## optimizer.zero_grad() ## takng outside loop\n",
    "    \n",
    "    images, targets, total_tp, all_IDs, feat_target  = data\n",
    "    #print(\"TRAIN: Feature shape out of the dataloader\", feat_target.shape)\n",
    "    #images, total_tp, ages = data\n",
    "    \n",
    "    #, targets\n",
    "    #print(\"image device\", images.device)\n",
    "    #print(\"Train: total time points in this sample\", total_tp)\n",
    "    #selected = [random.randint(2, value) for value in total_tp]\n",
    "    #print(\"selected time points in list\", selected)\n",
    "    idx = max(total_tp)\n",
    "    #print(\"selected time point\", idx)\n",
    "    #print(\"all images shape in training: \", images.cpu().numpy().shape)\n",
    "    #print(\"all targets shape in training: \", targets.cpu().numpy().shape)\n",
    "\n",
    "    \"\"\"\n",
    "    Train: total time points in this sample tensor([6, 5, 3, 6, 4, 4, 5, 8], dtype=torch.int32)\n",
    "    all images shape in training:  (8, 13, 1, 112, 88, 88)\n",
    "    all targets shape in training:  (8, 13, 3, 112, 88, 88)\n",
    "\n",
    "    all pred shape in training:  (8, 8, 3, 112, 88, 88)\n",
    "    all targets shape going into function:  (8, 8, 3, 112, 88, 88)\n",
    "    \"\"\"\n",
    "\n",
    "    # Assumes: images shape is [B, T, 1, D, H, W]\n",
    "    B, _, _, D, H, W = images.shape\n",
    "    all_preds = torch.zeros((B, idx, 3, D, H, W), device=images.device)\n",
    "    all_feat_pred = torch.zeros((B, idx, 256, 12, 12, 12), device=images.device)\n",
    "    \n",
    "    for bbb in range(B): \n",
    "        TPPP = total_tp[bbb]\n",
    "        for ttt in range(TPPP):  \n",
    "            img = images[bbb, ttt, :, :, :, :] \n",
    "            pred, cur_feat = model(img.unsqueeze(0)) \n",
    "            all_preds[bbb, ttt, :, :, :, :] = pred \n",
    "            all_feat_pred[bbb, ttt, :, :, :, :] = cur_feat\n",
    "\n",
    "    \n",
    "    #loss_1, dice1, dice2, all_of_them = criterion(all_preds[:2, :, :, :, :, :], all_f[:2, :, :, :, :, :], targets[:2, :idx, :, :, :, :], None, None, None, None, cur_fold, loss_name=\"loss1\")\n",
    "    loss_1, dice1, dice2, all_of_them = criterion(all_preds, all_feat_pred, targets[:, :idx, :, :, :, :], feat_target[:, :idx, :, :, :, :], None, total_tp, None, None, None, loss_name=\"loss1\")\n",
    "    \n",
    "    del cur_pred\n",
    "\n",
    "    sh = (images[:, :, :, :, :, :].cpu().numpy().shape[0], images[:, :, :, :, :, :].cpu().numpy().shape[1], 3)\n",
    "    get_volume = np.zeros(sh)\n",
    "    #print(\"volume record shape\", get_volume.shape)\n",
    "    for cur in range(0, max_tp, 1):\n",
    "        preds_cur, _ = model(images[:, cur, :, :, :, :])\n",
    "        probs_cur = torch.softmax(preds_cur, dim=1)\n",
    "        preds_cur = torch.argmax(probs_cur, dim=1).cpu().numpy()\n",
    "        #print(\"current pred dimension\", preds_cur.shape)\n",
    "        for b in range(0, get_volume.shape[0], 1):\n",
    "            #print(\"time point in cur\", cur, \"actual total tp\", total_tp[b])\n",
    "            if (cur<total_tp[b]):\n",
    "                get_volume[b][cur][0] = (preds_cur[b, :, :, :] == 0).sum()\n",
    "                get_volume[b][cur][1] = (preds_cur[b, :, :, :] == 1).sum()\n",
    "                get_volume[b][cur][2] = (preds_cur[b, :, :, :] == 2).sum()\n",
    "\n",
    "    del preds_cur, _, probs_cur\n",
    "    #print(get_volume)\n",
    "    #get_volume = (get_volume/2097152)\n",
    "    \n",
    "    loss_2, _, _, temp = criterion(None, None, None, None, get_volume, total_tp, ages, all_IDs, None, loss_name=\"loss2\")\n",
    "    all_of_them.append(temp)\n",
    "    del temp\n",
    "    \n",
    "    loss_3, _, _, temp = criterion(None, None, None, None, get_volume, total_tp, ages, all_IDs, None, loss_name=\"loss3\")\n",
    "    all_of_them.append(temp)\n",
    "    del temp\n",
    "\n",
    "    ICVs = []\n",
    "    diagnosis = []\n",
    "    \n",
    "    loss_4, _, _, temp = criterion(None, None, None, None, get_volume, total_tp, ages, all_IDs, cur_fold, loss_name=\"vol_loss\")\n",
    "    all_of_them.append(temp)\n",
    "    del temp\n",
    "\n",
    "    loss_5, _, _, temp = criterion(all_preds, None, None, None, None, total_tp, None, None, None, loss_name=\"sdf_loss\")\n",
    "    all_of_them.append(temp)\n",
    "    del _, temp\n",
    "    del all_preds\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    #print(\"unique in ground truth\", torch.unique(target))\n",
    "    #print(\"unique in prediction\", torch.unique((preds)))\n",
    "\n",
    "    loss = loss_1 + (0.5*loss_2) + (0.1*loss_3) + loss_4 + loss_5\n",
    "    print(\"Train: Loss 1\", loss_1.item(), \", Loss 2\", loss_2.item(), \", Loss 3\", loss_3.item(), \", Loss 4\", loss_4.item(), \", Loss 5\", loss_5.item())\n",
    "\n",
    "    all_of_them.append(loss.item())\n",
    "    loss_row = all_of_them\n",
    "    with open(loss_file_loc, 'a') as fl:\n",
    "        writer = csv.writer(fl)\n",
    "        writer.writerow(loss_row)\n",
    "    \n",
    "    #loss.backward()  ## takng outside loop\n",
    "    #optimizer.step()  ## takng outside loop\n",
    "\n",
    "    #for obj in gc.get_objects():\n",
    "    #    if torch.is_tensor(obj) and obj.is_cuda:\n",
    "    #        print(f\"Tensor: {obj.shape}, dtype: {obj.dtype}, device: {obj.device}\")\n",
    "        \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return loss, dice1.item(), dice2.item() \n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(model, data, criterion, loss_file_loc):\n",
    "\n",
    "    cur_fold = global_fold\n",
    "    print(\"################################## Val Loop: is fold\", cur_fold)\n",
    "    \n",
    "    #loss_file_loc = loss_file_loc.replace(\"_train_\", \"_val_\")\n",
    "    cur_pred = []\n",
    "    all_preds = []\n",
    "    ages = []\n",
    "    \n",
    "    model.eval()\n",
    "    images, targets, total_tp, all_IDs, feat_target  = data\n",
    "    #print(\"VAL: Feature shape out of the dataloader\", feat_target.shape)\n",
    "    \n",
    "    #print(ages, ICVs, diagnosis)\n",
    "    #images, total_tp, ages = data\n",
    "    \n",
    "    #, targets\n",
    "    #print(\"image device\", images.device)\n",
    "    #print(\"Val total time points in this sample\", total_tp)\n",
    "    #selected = [random.randint(2, value) for value in total_tp]\n",
    "    #print(\"selected time points in list\", selected)\n",
    "    idx = max(total_tp)\n",
    "    #print(\"selected time point\", idx)\n",
    "    #print(\"all images shape in validation: \", images[:, :, :, :, :, :].cpu().numpy().shape)\n",
    " \n",
    "    # Assumes: images shape is [B, T, 1, D, H, W]\n",
    "    B, _, _, D, H, W = images.shape\n",
    "    all_preds = torch.zeros((B, idx, 3, D, H, W), device=images.device)\n",
    "    all_feat_pred = torch.zeros((B, idx, 256, 12, 12, 12), device=images.device)\n",
    "    \n",
    "    for bbb in range(B): \n",
    "        TPPP = total_tp[bbb]\n",
    "        for ttt in range(TPPP):  \n",
    "            img = images[bbb, ttt, :, :, :, :] \n",
    "            pred, cur_feat = model(img.unsqueeze(0)) \n",
    "            all_preds[bbb, ttt, :, :, :, :] = pred \n",
    "            all_feat_pred[bbb, ttt, :, :, :, :] = cur_feat\n",
    "\n",
    "    #print(\"VAL: ALL IMAGE PREDICTION SIZE\", all_preds.shape)\n",
    "    #print(\"VAL: ALL FEATURE PREDICTION SIZE\", all_feat_pred.shape)\n",
    "\n",
    "    #loss_1, dice1, dice2, all_of_them = criterion(all_preds[:2, :, :, :, :, :], all_f[:2, :, :, :, :, :], targets[:2, :idx, :, :, :, :], None, None, None, None, None, loss_name=\"loss1\")\n",
    "    loss_1, dice1, dice2, all_of_them = criterion(all_preds, all_feat_pred, targets[:, :idx, :, :, :, :], feat_target[:, :idx, :, :, :, :], None, total_tp, None, None, None, loss_name=\"loss1\")\n",
    "    \n",
    "    del cur_pred\n",
    "\n",
    "    sh = (images[:, :, :, :, :, :].cpu().numpy().shape[0], images[:, :, :, :, :, :].cpu().numpy().shape[1], 3)\n",
    "    get_volume = np.zeros(sh)\n",
    "    #print(\"volume record shape\", get_volume.shape)\n",
    "    for cur in range(0, max_tp, 1):\n",
    "        preds_cur, _ = model(images[:, cur, :, :, :, :])\n",
    "        probs_cur = torch.softmax(preds_cur, dim=1)\n",
    "        preds_cur = torch.argmax(probs_cur, dim=1).cpu().numpy()\n",
    "        #print(\"current pred dimension\", preds_cur.shape)\n",
    "        for b in range(0, get_volume.shape[0], 1):\n",
    "            #print(\"time point in cur\", cur, \"actual total tp\", total_tp[b])\n",
    "            if (cur<total_tp[b]):\n",
    "                get_volume[b][cur][0] = (preds_cur[b, :, :, :] == 0).sum()\n",
    "                get_volume[b][cur][1] = (preds_cur[b, :, :, :] == 1).sum()\n",
    "                get_volume[b][cur][2] = (preds_cur[b, :, :, :] == 2).sum()\n",
    "\n",
    "    del preds_cur, _, probs_cur\n",
    "    #get_volume = (get_volume/2097152)\n",
    "    #print(get_volume)\n",
    "\n",
    "    loss_2, _, _, temp = criterion(None, None, None, None, get_volume, total_tp, ages, all_IDs, None, loss_name=\"loss2\")\n",
    "    all_of_them.append(temp)\n",
    "    del temp\n",
    "    \n",
    "    loss_3, _, _, temp = criterion(None, None, None, None, get_volume, total_tp, ages, all_IDs, None, loss_name=\"loss3\")\n",
    "    all_of_them.append(temp)\n",
    "    del temp\n",
    "\n",
    "    ICVs = []\n",
    "    diagnosis = []\n",
    "    \n",
    "    loss_4, _, _, temp = criterion(None, None, None, None, get_volume, total_tp, ages, all_IDs, cur_fold, loss_name=\"vol_loss\")\n",
    "    all_of_them.append(temp)\n",
    "    del temp\n",
    "\n",
    "    loss_5, _, _, temp = criterion(all_preds, None, None, None, None, total_tp, None, None, None, loss_name=\"sdf_loss\")\n",
    "    all_of_them.append(temp)\n",
    "    del temp\n",
    "    del all_preds\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "\n",
    "    loss = loss_1 + (0.5*loss_2) + (0.1*loss_3) + loss_4 + loss_5\n",
    "    print(\"Val: Loss 1\", loss_1.item(), \", Loss 2\", loss_2.item(), \", Loss 3\", loss_3.item(), \", Loss 4\", loss_4.item(), \", Loss 5\", loss_5.item())\n",
    "    \n",
    "    all_of_them.append(loss.item())\n",
    "    loss_row = all_of_them\n",
    "    with open(loss_file_loc, 'a') as fl:\n",
    "        writer = csv.writer(fl)\n",
    "        writer.writerow(loss_row)\n",
    "\n",
    "    #for obj in gc.get_objects():\n",
    "    #    if torch.is_tensor(obj) and obj.is_cuda:\n",
    "    #        print(f\"Tensor: {obj.shape}, dtype: {obj.dtype}, device: {obj.device}\")\n",
    "        \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return loss.item(), dice1.item(), dice2.item()\n",
    "\n",
    "patience = 20 #10  # Number of epochs to wait before stopping if no improvement\n",
    "early_stopping_counter = 0  # Counter for early stopping\n",
    "\n",
    "for fold in range(global_fold, global_fold+1, 1):\n",
    "\n",
    "    fold_dir_path = task_path + losses_used + '/models/fold-' + str(fold)\n",
    "    os.makedirs(fold_dir_path, exist_ok=True)\n",
    "\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    \n",
    "    print(\"############################################################################\")\n",
    "    print(\"Loading Data for fold \", fold)\n",
    "\n",
    "    train_dataset = mydata_nnunet(task_path + \"/\",'train', fold)\n",
    "    val_dataset = mydata_nnunet(task_path + \"/\",'valid', fold)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 4, shuffle = True, collate_fn=train_dataset.collate)  \n",
    "    val_loader = DataLoader(val_dataset, batch_size = 4, shuffle = False, collate_fn=val_dataset.collate) \n",
    "\n",
    "    print(f\"Train Loader {len(train_loader)} : Val Loader {len(val_loader)}\")\n",
    "    print(\"############################################################################\")\n",
    "\n",
    "    ### model = Unet3D(1, 3, 16) \n",
    "\n",
    "    ### LOADING FOR THIS LONGIT\n",
    "    pretrained_model = nnUNet3D(in_channels=1, out_channels=3)  #Unet3D(1, 3, 16) \n",
    "\n",
    "    ## for the vol loss function - crop ONLY DS is enough\n",
    "    # should not need a registered dataset\n",
    "    best_model_path = 'PATH_TO_PRETRAIN/models/fold-' + str(fold) + '/model_fold_' + str(fold) + '_best_model.pth'\n",
    "\n",
    "    #### Loading the model\n",
    "    #model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "    pretrained_model.load_state_dict(torch.load(best_model_path, weights_only=True), strict=False)\n",
    "    print(\"Model Loaded:\", best_model_path) \n",
    "    \n",
    "    # Initialize new model with Dropout\n",
    "    model = nnUNet3D_Dropout(in_channels=1, out_channels=3)  #Unet3D_Dropout(in_dim=1, out_dim=3, num_filters=16, dropout_rate=0.4)\n",
    "    # Transfer Weights (Ignoring Mismatched Layers)\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    \n",
    "    \n",
    "    # Wrap the model with DataParallel to use multiple GPUs\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"multiple gpus identified\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    \n",
    "    # Move model to GPU\n",
    "    model = model.cuda()\n",
    "    \n",
    "    initial_lr = 0.005 #0.005  \n",
    "    nEpochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    print(\"############################################################################\")\n",
    "    print(\"Running for total\", nEpochs, \"epochs\")\n",
    "    print(\"############################################################################\")\n",
    "\n",
    "    \n",
    "    # optimser\n",
    "    #opt = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=1e-5) # original longit\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=1e-5) # <--- one that worked for cross\n",
    "    \n",
    "    # loss function\n",
    "    criterion = get_loss()\n",
    "    \n",
    "    # scheduler\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5) #original longit\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.8, patience=3, verbose=True) # <--- one that worked for cross\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.7, patience=3, verbose=True) \n",
    "\n",
    "    \n",
    "    loss_file_loc = task_path + losses_used + \"/all_losses_train_fold_\" + str(fold) + \".csv\"\n",
    "    \n",
    "    with open(loss_file_loc, 'w') as fl:\n",
    "        \n",
    "        writer = csv.writer(fl)\n",
    "        writer.writerow(fields)\n",
    "\n",
    "    with open(loss_file_loc.replace(\"all_losses_train_\", \"all_losses_val_\"), 'w') as fl:\n",
    "        \n",
    "        writer = csv.writer(fl)\n",
    "        writer.writerow(fields)\n",
    "\n",
    "    with open(task_path + losses_used + \"/all_losses_iter_train_fold_\" + str(fold) + \".csv\", 'w') as fl:\n",
    "        writer = csv.writer(fl)\n",
    "        writer.writerow(temp_fields)\n",
    "\n",
    "    with open(task_path + losses_used + \"/all_losses_iter_val_fold_\" + str(fold) + \".csv\", 'w') as fl:\n",
    "        writer = csv.writer(fl)\n",
    "        writer.writerow(temp_fields)\n",
    "\n",
    "    mlflow.set_experiment(task_name_exp)\n",
    "    # enable logging of system metrics such as CPU usage, memory usage, disk I/O, and network I/O to MLflow runs -> monitoring resource utilization\n",
    "    #mlflow.enable_system_metrics_logging()\n",
    "    run_name = task_name_exp + \"_fold_\" + str(fold) + \"_epochs_\" + str(nEpochs) + '_time_' + str(now)\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name, log_system_metrics=True) as run:\n",
    "        \n",
    "        for epoch in tqdm(range(nEpochs)):\n",
    "    \n",
    "            with open(task_path + losses_used + \"/temp_train_fold_\" + str(fold) + \".csv\", 'w') as fl:\n",
    "                writer = csv.writer(fl)\n",
    "                writer.writerow(temp_fields)\n",
    "    \n",
    "            with open(task_path + losses_used + \"/temp_val_fold_\" + str(fold) + \".csv\", 'w') as fl:\n",
    "                writer = csv.writer(fl)\n",
    "                writer.writerow(temp_fields)\n",
    "    \n",
    "            train_loss = val_loss = 0\n",
    "            dice_1_t = dice_2_t= dice_3_t = dice_1_v = dice_2_v = dice_3_v = 0\n",
    "        \n",
    "            print(\"Epoch \", epoch, \":\")\n",
    "\n",
    "\n",
    "            accum_steps = 2  # for example\n",
    "            opt.zero_grad()  \n",
    "\n",
    "            for i, data in enumerate(train_loader):\n",
    "\n",
    "                #data = data\n",
    "                #print(data.device)\n",
    "                loss, dice1, dice2 = train_batch(model, data, opt, criterion, loss_file_loc=task_path + losses_used + \"/temp_train_fold_\" + str(fold) + \".csv\")\n",
    "                (loss / accum_steps).backward() # Scale loss by accumulation steps to keep gradients consistent\n",
    "                train_loss += loss.item()\n",
    "                dice_1_t += dice1\n",
    "                dice_2_t += dice2\n",
    "\n",
    "                 # Perform optimizer step and zero_grad every accum_steps batches OR at the last batch\n",
    "                if (i + 1) % accum_steps == 0 or (i + 1) == len(train_loader):\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "                    \n",
    "            d = len(train_loader)\n",
    "            avg_train = f\"Train : Overall Loss: {train_loss/d} | L1 Dice : {(dice_1_t/d)*100}% | L2 Dice : {(dice_2_t/d)*100}%\"\n",
    "            #print(avg_train)\n",
    "        \n",
    "            for i_v, data_v in enumerate(val_loader):\n",
    "                #data_v = data_v\n",
    "                loss, dice1, dice2 = validate_batch(model, data_v, criterion, loss_file_loc=task_path + losses_used + \"/temp_val_fold_\" + str(fold) + \".csv\")\n",
    "                val_loss += loss\n",
    "                dice_1_v += dice1\n",
    "                dice_2_v += dice2\n",
    "            d_v = len(val_loader)\n",
    "            avg_val = f\"Val : Overall Loss: {val_loss/d_v} | L1 Dice : {(dice_1_v/d_v)*100}% | L2 Dice : {(dice_2_v/d_v)*100}%\"\n",
    "            #print(avg_val)\n",
    "\n",
    "            mlflow.log_metric('Train Total Loss', train_loss/d, step=epoch)\n",
    "            mlflow.log_metric('Val Total Loss', val_loss/d_v, step=epoch)\n",
    "            \n",
    "\n",
    "            # Print the learning rate\n",
    "            for param_group in opt.param_groups:\n",
    "                current_lr = param_group['lr']\n",
    "            print(f\"Learning rate: {current_lr}\")\n",
    "\n",
    "            #mlflow.log_metric('Learning_rate', current_lr, step=epoch)\n",
    "\n",
    "            \n",
    "            get_train_avg = get_average_from_csv(task_path + losses_used + \"/temp_train_fold_\" + str(fold) + \".csv\")\n",
    "            get_val_avg = get_average_from_csv(task_path + losses_used + \"/temp_val_fold_\" + str(fold) + \".csv\")\n",
    "            #print(\"############################################################################\")\n",
    "            get_train_avg = np.append(get_train_avg, epoch)\n",
    "            get_train_avg = np.append(get_train_avg, current_lr)\n",
    "            #print(get_train_avg)\n",
    "            #print(\"############################################################################\")\n",
    "            get_val_avg = np.append(get_val_avg, epoch)\n",
    "            get_val_avg = np.append(get_val_avg, current_lr)\n",
    "            #print(get_val_avg)\n",
    "            #print(\"############################################################################\")\n",
    "            # Load the new data (without header)\n",
    "            new_data_train = pd.read_csv(task_path + losses_used + \"/temp_train_fold_\" + str(fold) + \".csv\", header=None, skiprows=1)\n",
    "            new_data_train.to_csv(task_path + losses_used + \"/all_losses_iter_train_fold_\" + str(fold) + \".csv\", mode=\"a\", header=False, index=False)\n",
    "            #print(\"temp train:\", new_data_train)\n",
    "            new_data_val = pd.read_csv(task_path + losses_used + \"/temp_val_fold_\" + str(fold) + \".csv\", header=None, skiprows=1)\n",
    "            new_data_val.to_csv(task_path + losses_used + \"/all_losses_iter_val_fold_\" + str(fold) + \".csv\", mode=\"a\", header=False, index=False)\n",
    "            #print(\"temp val:\", new_data_val)\n",
    "\n",
    "            for p in range(0, len(fields), 1):\n",
    "                #print(\"ML logging:\", fields[p], get_train_avg[p], epoch)\n",
    "                mlflow.log_metric(\"Train \" + fields[p], get_train_avg[p], step=epoch)\n",
    "\n",
    "            for p in range(0, len(fields), 1):\n",
    "                mlflow.log_metric(\"Val \" + fields[p], get_val_avg[p], step=epoch)\n",
    "                \n",
    "            with open(loss_file_loc, 'a') as fl:\n",
    "                writer = csv.writer(fl)\n",
    "                writer.writerow(get_train_avg)\n",
    "    \n",
    "            with open(loss_file_loc.replace(\"all_losses_train_\", \"all_losses_val_\"), 'a') as fl:\n",
    "                writer = csv.writer(fl)\n",
    "                writer.writerow(get_val_avg)\n",
    "\n",
    "            \"\"\"\n",
    "            ##### Condition of saving the model and early stopping the training\n",
    "            if best_val_loss>(val_loss/d_v) and (val_loss/d_v)>0:\n",
    "        \n",
    "                print(avg_train)\n",
    "                print(avg_val)\n",
    "                early_stopping_counter = 0 \n",
    "                file_name = task_path + losses_used + '/models/model_fold_' + str(fold) + '_best_model.pth'\n",
    "                print(\"Saving Model\")\n",
    "                torch.save(model.state_dict(), file_name)\n",
    "                torch.save(opt.state_dict(), file_name.replace('best_model.pth', 'optimizer_state.pth'))\n",
    "                torch.save(scheduler.state_dict(), file_name.replace('best_model.pth', 'scheduler_state.pth'))\n",
    "                best_val_loss = (val_loss/d_v)\n",
    "                log_epoch(epoch)\n",
    "                \n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                print(f\"No improvement. Early stopping counter: {early_stopping_counter}/{patience}\")\n",
    "            \n",
    "\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "            \"\"\"\n",
    "\n",
    "            #### Saving all models and no early stopping\n",
    "            print(avg_train)\n",
    "            print(avg_val)\n",
    "            file_name = task_path + losses_used + '/models/fold-' + str(fold) + '/model_fold_' + str(fold) + '_best_model_epoch_' + str(epoch) + '.pth'\n",
    "            print(\"Saving Model as \", file_name)\n",
    "            torch.save(model.state_dict(), file_name)\n",
    "            torch.save(opt.state_dict(), file_name.replace('best_model_', 'optimizer_state_'))\n",
    "            torch.save(scheduler.state_dict(), file_name.replace('best_model_', 'scheduler_state_'))\n",
    "            best_val_loss = (val_loss/d_v)\n",
    "            log_epoch(epoch)\n",
    "            \n",
    "            \n",
    "            # Step the scheduler at every epoch\n",
    "            scheduler.step(val_loss/d_v)\n",
    "            #scheduler.step()\n",
    "    \n",
    "            # Simulating updated losses\n",
    "            all_train_losses.append((train_loss/d))  \n",
    "            all_val_losses.append((val_loss/d_v)) \n",
    "            #print(\"train losses\", all_train_losses)\n",
    "            #print(\"val losses\", all_val_losses)\n",
    "            #print(len((np.array(all_train_losses))), len((np.array(all_val_losses)))) \n",
    "            \n",
    "            # Plot and save current losses\n",
    "            plot_all(loss_file_loc, fold)\n",
    "            plot_all_iter(task_path + losses_used + \"/all_losses_iter_train_fold_\" + str(fold) + \".csv\", fold, \"train\")\n",
    "            plot_all_iter(task_path + losses_used + \"/all_losses_iter_val_fold_\" + str(fold) + \".csv\", fold, \"val\")\n",
    "            #plot_and_save_losses(np.array(all_train_losses), np.array(all_val_losses), fold)\n",
    "\n",
    "            \n",
    "    # Log the model\n",
    "    mlflow.pytorch.log_model(model, task_name_exp)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "#now = datetime.now()\n",
    "now = datetime.datetime.now()\n",
    "print(\"TRAINING DONE !\")\n",
    "print(\"Terminated at:\", now)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f183db-2828-4121-a6fa-9d9189fa6618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da8ac4-128a-4598-94af-8ba57eca3591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585069e-75d3-430b-8d22-26304d415a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jan2025-env1",
   "language": "python",
   "name": "jan2025-env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
